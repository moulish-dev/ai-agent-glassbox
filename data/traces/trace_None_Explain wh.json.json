{
  "user_query": "Explain why world models are important for AI safety in simple terms.",
  "memory": {
    "plan": "[OLLAMA ERROR] HTTPConnectionPool(host='localhost', port=11434): Read timed out. (read timeout=60) | PROMPT: You are a research agent. User query: Explain why world models are important for AI safety in simple terms.. Decide if you need web search.",
    "research_raw": "[SEARCH RESULTS for: Explain why world models are important for AI safety in simple terms.] ---> What are AI \u2018world models,\u2019 and why do they matter?\n\nWorld models, also known as world simulators, are being touted by some as the next big thing in AI.\n\nAI pioneer Fei-Fei Li\u2019s [World Labs](https://techcrunch.com/2024/09/13/fei-fei-lis-world-labs-comes-out-of-stealth-with-230m-in-funding/) has raised $230 million to build \u201clarge world models,\u201d and DeepMind [hired](https://techcrunch.com/2024/10/03/a-co-lead-on-sora-openais-video-generator-has-left-for-google/) one of the creators of OpenAI\u2019s video generator, [Sora](https://techcrunch.com/2024/02/15/openais-newest-model-can-generate-videos-and-they-look-decent/), to work on \u201cworld simulators.\u201d (Sora was released on Monday; [here are some early impressions](https://techcrunch.com/2024/12/09/openais-sora-is-launching-today-heres-highlights-from-the-first-review/).)\n\nBut what the heck *are*these things?\n\nWorld models take inspiration from the mental models of the world that humans develop naturally. Our brains take the abstract representations from our senses and form them into more concrete understanding of the world around us, producing what we called \u201cmodels\u201d long before AI adopted the phrase. The predictions our brains make based on these models influence how we perceive the world.\n\nA [paper](https://arxiv.org/pdf/1803.10122) by AI researchers David Ha and J\u00fcrgen Schmidhuber gives the example of a baseball batter. Batters have milliseconds to decide how to swing their bat \u2014 shorter than the time it takes for visual signals to reach the brain. The reason they\u2019re able to hit a 100-mile-per-hour fastball is because they can instinctively predict where the ball will go, Ha and Schmidhuber say.\n\n\u201cFor professional players, this all happens subconsciously,\u201d the research duo writes. \u201cTheir muscles reflexively swing the bat at the right time and location in line with their internal models\u2019 predictions. They can quickly act on their predictions of the future without the need to consciously roll out possible future scenarios to form a plan.\u201d\n\nIt\u2019s these subconscious reasoning aspects of world models that some believe are prerequisites for human-level intelligence.\n\nTechcrunch event\n\n### Join the Disrupt 2026 Waitlist\n\n#### Add yourself to the Disrupt 2026 waitlist to be first in line when Early Bird tickets drop. Past Disrupts have brought Google Cloud, Netflix, Microsoft, Box, Phia, a16z, ElevenLabs, Wayve, Hugging Face, Elad Gil, and Vinod Khosla to the stages \u2014 part of 250+ industry leaders driving 200+ sessions built to fuel your growth and sharpen your edge. Plus, meet the hundreds of startups innovating across every sector.\n\n### Join the Disrupt 2026 Waitlist\n\n#### Add yourself to the Disrupt 2026 waitlist to be first in line when Early Bird tickets drop. Past Disrupts have brought Google Cloud, Netflix, Microsoft, Box, Phia, a16z, ElevenLabs, Wayve, Hugging Face, Elad Gil, and Vinod Khosla to the stages \u2014 part of 250+ industry leaders driving 200+ sessions built to fuel your growth and sharpen your edge. Plus, meet the hundreds of startups innovating across every sector.\n\nSan Francisco | October 13-15, 2026\n\nWAITLIST NOW\n\n## Modeling the world\n\nWhile the concept has been around for decades, world models have gained popularity recently in part because of their promising applications in the field of generative video.\n\nMost, if not all, AI-generated videos veer into uncanny valley territory. Watch them long enough and somethingbizarrewill happen, like limbs twisting and merging into each other.\n\nWhile a generative model trained on years of video might accurately predict that a basketball bounces, it doesn\u2019t actually have any idea why \u2014 just like language models don\u2019t really understand the concepts behind words and phrases. But a world model with even a basic grasp of why the basketball bounces like it does will be better at showing it do that thing.\n\nTo enable this kind of insight, world models are trained on a range of data, including photos, audio, videos, and text, with the intent of creating internal representations of how the world works, and the ability to reason about the consequences of actions.\n\nA sample from AI startup Runway\u2019s Gen-3 video generation model.Image Credits:Runway\n\n\u201cA viewer expects that the world they\u2019re watching behaves in a similar way to their reality,\u201d Alex Mashrabov, Snap\u2019s ex-AI chief of AI and the CEO of [Higgsfield](https://techcrunch.com/2024/04/03/former-snap-ai-chief-launches-higgsfield-to-take-on-openais-sora-video-generator/), which is building generative models for video, said. \u201cIf a feather drops with the weight of an anvil or a bowling ball shoots up hundreds of feet into the air, it\u2019s jarring and takes the viewer out of the moment. With a strong world model, instead of a creator defining how each object is expected to move \u2014 which is tedious, cumbersome, and a poor use of time \u2014 the model will understand this.\u201d\n\nBut better video generation is only the tip of the iceberg for world models. Researchers including Meta chief AI scientist Yann LeCun say the models could someday be used for sophisticated forecasting and planning in both the digital and physical realm.\n\nIn a [talk](https://techcrunch.com/2024/10/16/metas-ai-chief-says-world-models-are-key-to-human-level-ai-but-it-might-be-10-years-out/) earlier this year, LeCun described how a world model could help achieve a desired goal through reasoning. A model with a base representation of a \u201cworld\u201d (e.g. a video of a dirty room), given an objective (a clean room), could come up with a sequence of actions to achieve that objective (deploy vacuums to sweep, clean the dishes, empty the trash) not because that\u2019s a pattern it has observed but because it knows at a deeper level how to go from dirty to clean.\n\n\u201cWe need machines that understand the world; [machines] that can remember things, that have intuition, have common sense \u2014 things that can reason and plan to the same level as humans,\u201d LeCun said. \u201cDespite what you might have heard from some of the most enthusiastic people, current AI systems are not capable of any of this.\u201d\n\nWhile LeCun estimates that we\u2019re at least a decade away from the world models he envisions, today\u2019s world models are showing promise as elementary physics simulators.\n\nSora controlling a player in Minecraft \u2014 and rendering the world.Image Credits:OpenAI\n\nOpenAI notes in a blog that Sora, which it considers to be a world model, can simulate actions like a painter leaving brush strokes on a canvas. Models like Sora \u2014 and Sora [itself](https://techcrunch.com/2024/02/15/openais-sora-video-generating-model-can-render-video-games-too/) \u2014 can also effectively [simulate](https://arxiv.org/abs/1803.10122) [video](https://arxiv.org/abs/2005.12126) [games](https://arstechnica.com/gadgets/2024/03/googles-genie-model-creates-interactive-2d-worlds-from-a-single-image/). For example, Sora can render a Minecraft-like UI and game world.\n\nFuture world models may be able to generate 3D worlds on demand for gaming, virtual photography, and more, World Labs co-founder Justin Johnson said on an [episode](https://a16z.com/podcast/the-frontier-of-spatial-intelligence-with-fei-fei-li/) of the a16z podcast.\n\n\u201cWe already have the ability to create virtual, interactive worlds, but it costs hundreds and hundreds of millions of dollars and a ton of development time,\u201d Johnson said. \u201c[World models] will let you not just get an image or a clip out, but a fully simulated, vibrant, and interactive 3D world.\u201d\n\n## High hurdles\n\nWhile the concept is enticing, many technical challenges stand in the way.\n\nTraining and running world models requires massive compute power even compared to the amount currently used by generative models. While some of the latest language models can run on a modern smartphone, Sora (arguably an early world model) would require thousands of GPUs to train and run, especially if their use becomes commonplace.\n\nWorld models, like all AI models, also [hallucinate](https://techcrunch.com/2024/08/14/study-suggests-that-even-the-best-ai-models-hallucinate-a-bunch/) \u2014 and internalize biases in their training data. A world model trained largely on videos of sunny weather in European cities might struggle to comprehend or depict Korean cities in snowy conditions, for example, or simply do so incorrectly.\n\nA general lack of training data threatens to exacerbate these issues, says Mashrabov.\n\n\u201cWe have seen models being really limited with generations of people of a certain type or race,\u201d he said. \u201cTraining data for a world model must be broad enough to cover a diverse set of scenarios, but also highly specific to where the AI can deeply understand the nuances of those scenarios.\u201d\n\nIn a recent [post](https://runwayml.com/research/introducing-general-world-models), AI startup Runway\u2019s CEO, Crist\u00f3bal Valenzuela, says that data and engineering issues prevent today\u2019s models from accurately capturing the behavior of a world\u2019s inhabitants (e.g. humans and animals). \u201cModels will need to generate consistent maps of the environment,\u201d he said, \u201cand the ability to navigate and interact in those environments.\u201d\n\nA Sora-generated video.Image Credits:OpenAI\n\nIf all the major hurdles are overcome, though, Mashrabov believes that world models could \u201cmore robustly\u201d bridge AI with the real world \u2014 leading to breakthroughs not only in virtual world generation but robotics and AI decision-making.\n\nThey could also spawn more capable robots.\n\nRobots today are limited in what they can do because they don\u2019t have an awareness of the world around them (or their own bodies). World models could give them that awareness, Mashrabov said \u2014 at least to a point.\n\n\u201cWith an advanced world model, an AI could develop a personal understanding of whatever scenario it\u2019s placed in,\u201d he said, \u201cand start to reason out possible solutions.\u201d\n\n*TechCrunch has an AI-focused newsletter!**Sign up here**to get it in your inbox every Wednesday.*\n\n*This story originally published October 28, 2024, and was updated December 14, 2024, with new updates about Sora.*\n\nTopics AI, Artificial Intelligence (AI), evergreens, world models\n\nKyle Wiggers AI Editor Kyle Wiggers was TechCrunch\u2019s AI Editor until June 2025. His writing has appeared in VentureBeat and Digital Trends, as well as a range of gadget blogs including Android Police, Android Authority, Droid-Life, and XDA-Developers. He lives in Manhattan with his partner, a music therapist. View Bio\n\nDecember 3, 2025 Palo Alto, CA StrictlyVC concludes its 2025 series with an exclusive event featuring insights from leading VCs and builders such as Pat Gelsinger, Mina Fahmi, and more. Plus, opportunities to forge meaningful connections.Early Bird rate ends November 17. Register Now\n\n## Most Popular\n\n- Leaked documents shed light into how much OpenAI pays Microsoft Rebecca Bellan - Spotify introduces a Premium Platinum plan with lossless access in five markets Ivan Mehta - Jack Dorsey funds diVine, a Vine reboot that includes Vine\u2019s video archive Sarah Perez - \u2018Chad: The Brainrot IDE\u2019 is a new Y Combinator-backed product so wild, people thought it was fake Julie Bort - Apple launches Digital ID, a way to carry your passport on your phone for use at TSA checkpoints Sarah Perez - Teradar raises $150M for a sensor it says beats lidar and radar Sean O'Kane - Meta\u2019s chief AI scientist Yann LeCun reportedly plans to leave to build his own startup Ram Iyer"
  },
  "steps": [
    {
      "step_id": 1,
      "node": "plan",
      "thought": "[OLLAMA ERROR] HTTPConnectionPool(host='localhost', port=11434): Read timed out. (read timeout=60) | PROMPT: You are a research agent. User query: Explain why world models are important for AI safety in simple terms.. Decide if you need web search.",
      "tool": null,
      "tool_input": null,
      "tool_output": null,
      "memory_before": {},
      "memory_after": {
        "plan": "[OLLAMA ERROR] HTTPConnectionPool(host='localhost', port=11434): Read timed out. (read timeout=60) | PROMPT: You are a research agent. User query: Explain why world models are important for AI safety in simple terms.. Decide if you need web search."
      }
    },
    {
      "step_id": 2,
      "node": "research",
      "thought": "Using web_search_tool based on plan.",
      "tool": "web_search_tool",
      "tool_input": "Explain why world models are important for AI safety in simple terms.",
      "tool_output": "[SEARCH RESULTS for: Explain why world models are important for AI safety in simple terms.] ---> What are AI \u2018world models,\u2019 and why do they matter?\n\nWorld models, also known as world simulators, are being touted by some as the next big thing in AI.\n\nAI pioneer Fei-Fei Li\u2019s [World Labs](https://techcrunch.com/2024/09/13/fei-fei-lis-world-labs-comes-out-of-stealth-with-230m-in-funding/) has raised $230 million to build \u201clarge world models,\u201d and DeepMind [hired](https://techcrunch.com/2024/10/03/a-co-lead-on-sora-openais-video-generator-has-left-for-google/) one of the creators of OpenAI\u2019s video generator, [Sora](https://techcrunch.com/2024/02/15/openais-newest-model-can-generate-videos-and-they-look-decent/), to work on \u201cworld simulators.\u201d (Sora was released on Monday; [here are some early impressions](https://techcrunch.com/2024/12/09/openais-sora-is-launching-today-heres-highlights-from-the-first-review/).)\n\nBut what the heck *are*these things?\n\nWorld models take inspiration from the mental models of the world that humans develop naturally. Our brains take the abstract representations from our senses and form them into more concrete understanding of the world around us, producing what we called \u201cmodels\u201d long before AI adopted the phrase. The predictions our brains make based on these models influence how we perceive the world.\n\nA [paper](https://arxiv.org/pdf/1803.10122) by AI researchers David Ha and J\u00fcrgen Schmidhuber gives the example of a baseball batter. Batters have milliseconds to decide how to swing their bat \u2014 shorter than the time it takes for visual signals to reach the brain. The reason they\u2019re able to hit a 100-mile-per-hour fastball is because they can instinctively predict where the ball will go, Ha and Schmidhuber say.\n\n\u201cFor professional players, this all happens subconsciously,\u201d the research duo writes. \u201cTheir muscles reflexively swing the bat at the right time and location in line with their internal models\u2019 predictions. They can quickly act on their predictions of the future without the need to consciously roll out possible future scenarios to form a plan.\u201d\n\nIt\u2019s these subconscious reasoning aspects of world models that some believe are prerequisites for human-level intelligence.\n\nTechcrunch event\n\n### Join the Disrupt 2026 Waitlist\n\n#### Add yourself to the Disrupt 2026 waitlist to be first in line when Early Bird tickets drop. Past Disrupts have brought Google Cloud, Netflix, Microsoft, Box, Phia, a16z, ElevenLabs, Wayve, Hugging Face, Elad Gil, and Vinod Khosla to the stages \u2014 part of 250+ industry leaders driving 200+ sessions built to fuel your growth and sharpen your edge. Plus, meet the hundreds of startups innovating across every sector.\n\n### Join the Disrupt 2026 Waitlist\n\n#### Add yourself to the Disrupt 2026 waitlist to be first in line when Early Bird tickets drop. Past Disrupts have brought Google Cloud, Netflix, Microsoft, Box, Phia, a16z, ElevenLabs, Wayve, Hugging Face, Elad Gil, and Vinod Khosla to the stages \u2014 part of 250+ industry leaders driving 200+ sessions built to fuel your growth and sharpen your edge. Plus, meet the hundreds of startups innovating across every sector.\n\nSan Francisco | October 13-15, 2026\n\nWAITLIST NOW\n\n## Modeling the world\n\nWhile the concept has been around for decades, world models have gained popularity recently in part because of their promising applications in the field of generative video.\n\nMost, if not all, AI-generated videos veer into uncanny valley territory. Watch them long enough and somethingbizarrewill happen, like limbs twisting and merging into each other.\n\nWhile a generative model trained on years of video might accurately predict that a basketball bounces, it doesn\u2019t actually have any idea why \u2014 just like language models don\u2019t really understand the concepts behind words and phrases. But a world model with even a basic grasp of why the basketball bounces like it does will be better at showing it do that thing.\n\nTo enable this kind of insight, world models are trained on a range of data, including photos, audio, videos, and text, with the intent of creating internal representations of how the world works, and the ability to reason about the consequences of actions.\n\nA sample from AI startup Runway\u2019s Gen-3 video generation model.Image Credits:Runway\n\n\u201cA viewer expects that the world they\u2019re watching behaves in a similar way to their reality,\u201d Alex Mashrabov, Snap\u2019s ex-AI chief of AI and the CEO of [Higgsfield](https://techcrunch.com/2024/04/03/former-snap-ai-chief-launches-higgsfield-to-take-on-openais-sora-video-generator/), which is building generative models for video, said. \u201cIf a feather drops with the weight of an anvil or a bowling ball shoots up hundreds of feet into the air, it\u2019s jarring and takes the viewer out of the moment. With a strong world model, instead of a creator defining how each object is expected to move \u2014 which is tedious, cumbersome, and a poor use of time \u2014 the model will understand this.\u201d\n\nBut better video generation is only the tip of the iceberg for world models. Researchers including Meta chief AI scientist Yann LeCun say the models could someday be used for sophisticated forecasting and planning in both the digital and physical realm.\n\nIn a [talk](https://techcrunch.com/2024/10/16/metas-ai-chief-says-world-models-are-key-to-human-level-ai-but-it-might-be-10-years-out/) earlier this year, LeCun described how a world model could help achieve a desired goal through reasoning. A model with a base representation of a \u201cworld\u201d (e.g. a video of a dirty room), given an objective (a clean room), could come up with a sequence of actions to achieve that objective (deploy vacuums to sweep, clean the dishes, empty the trash) not because that\u2019s a pattern it has observed but because it knows at a deeper level how to go from dirty to clean.\n\n\u201cWe need machines that understand the world; [machines] that can remember things, that have intuition, have common sense \u2014 things that can reason and plan to the same level as humans,\u201d LeCun said. \u201cDespite what you might have heard from some of the most enthusiastic people, current AI systems are not capable of any of this.\u201d\n\nWhile LeCun estimates that we\u2019re at least a decade away from the world models he envisions, today\u2019s world models are showing promise as elementary physics simulators.\n\nSora controlling a player in Minecraft \u2014 and rendering the world.Image Credits:OpenAI\n\nOpenAI notes in a blog that Sora, which it considers to be a world model, can simulate actions like a painter leaving brush strokes on a canvas. Models like Sora \u2014 and Sora [itself](https://techcrunch.com/2024/02/15/openais-sora-video-generating-model-can-render-video-games-too/) \u2014 can also effectively [simulate](https://arxiv.org/abs/1803.10122) [video](https://arxiv.org/abs/2005.12126) [games](https://arstechnica.com/gadgets/2024/03/googles-genie-model-creates-interactive-2d-worlds-from-a-single-image/). For example, Sora can render a Minecraft-like UI and game world.\n\nFuture world models may be able to generate 3D worlds on demand for gaming, virtual photography, and more, World Labs co-founder Justin Johnson said on an [episode](https://a16z.com/podcast/the-frontier-of-spatial-intelligence-with-fei-fei-li/) of the a16z podcast.\n\n\u201cWe already have the ability to create virtual, interactive worlds, but it costs hundreds and hundreds of millions of dollars and a ton of development time,\u201d Johnson said. \u201c[World models] will let you not just get an image or a clip out, but a fully simulated, vibrant, and interactive 3D world.\u201d\n\n## High hurdles\n\nWhile the concept is enticing, many technical challenges stand in the way.\n\nTraining and running world models requires massive compute power even compared to the amount currently used by generative models. While some of the latest language models can run on a modern smartphone, Sora (arguably an early world model) would require thousands of GPUs to train and run, especially if their use becomes commonplace.\n\nWorld models, like all AI models, also [hallucinate](https://techcrunch.com/2024/08/14/study-suggests-that-even-the-best-ai-models-hallucinate-a-bunch/) \u2014 and internalize biases in their training data. A world model trained largely on videos of sunny weather in European cities might struggle to comprehend or depict Korean cities in snowy conditions, for example, or simply do so incorrectly.\n\nA general lack of training data threatens to exacerbate these issues, says Mashrabov.\n\n\u201cWe have seen models being really limited with generations of people of a certain type or race,\u201d he said. \u201cTraining data for a world model must be broad enough to cover a diverse set of scenarios, but also highly specific to where the AI can deeply understand the nuances of those scenarios.\u201d\n\nIn a recent [post](https://runwayml.com/research/introducing-general-world-models), AI startup Runway\u2019s CEO, Crist\u00f3bal Valenzuela, says that data and engineering issues prevent today\u2019s models from accurately capturing the behavior of a world\u2019s inhabitants (e.g. humans and animals). \u201cModels will need to generate consistent maps of the environment,\u201d he said, \u201cand the ability to navigate and interact in those environments.\u201d\n\nA Sora-generated video.Image Credits:OpenAI\n\nIf all the major hurdles are overcome, though, Mashrabov believes that world models could \u201cmore robustly\u201d bridge AI with the real world \u2014 leading to breakthroughs not only in virtual world generation but robotics and AI decision-making.\n\nThey could also spawn more capable robots.\n\nRobots today are limited in what they can do because they don\u2019t have an awareness of the world around them (or their own bodies). World models could give them that awareness, Mashrabov said \u2014 at least to a point.\n\n\u201cWith an advanced world model, an AI could develop a personal understanding of whatever scenario it\u2019s placed in,\u201d he said, \u201cand start to reason out possible solutions.\u201d\n\n*TechCrunch has an AI-focused newsletter!**Sign up here**to get it in your inbox every Wednesday.*\n\n*This story originally published October 28, 2024, and was updated December 14, 2024, with new updates about Sora.*\n\nTopics AI, Artificial Intelligence (AI), evergreens, world models\n\nKyle Wiggers AI Editor Kyle Wiggers was TechCrunch\u2019s AI Editor until June 2025. His writing has appeared in VentureBeat and Digital Trends, as well as a range of gadget blogs including Android Police, Android Authority, Droid-Life, and XDA-Developers. He lives in Manhattan with his partner, a music therapist. View Bio\n\nDecember 3, 2025 Palo Alto, CA StrictlyVC concludes its 2025 series with an exclusive event featuring insights from leading VCs and builders such as Pat Gelsinger, Mina Fahmi, and more. Plus, opportunities to forge meaningful connections.Early Bird rate ends November 17. Register Now\n\n## Most Popular\n\n- Leaked documents shed light into how much OpenAI pays Microsoft Rebecca Bellan - Spotify introduces a Premium Platinum plan with lossless access in five markets Ivan Mehta - Jack Dorsey funds diVine, a Vine reboot that includes Vine\u2019s video archive Sarah Perez - \u2018Chad: The Brainrot IDE\u2019 is a new Y Combinator-backed product so wild, people thought it was fake Julie Bort - Apple launches Digital ID, a way to carry your passport on your phone for use at TSA checkpoints Sarah Perez - Teradar raises $150M for a sensor it says beats lidar and radar Sean O'Kane - Meta\u2019s chief AI scientist Yann LeCun reportedly plans to leave to build his own startup Ram Iyer",
      "memory_before": {
        "plan": "[OLLAMA ERROR] HTTPConnectionPool(host='localhost', port=11434): Read timed out. (read timeout=60) | PROMPT: You are a research agent. User query: Explain why world models are important for AI safety in simple terms.. Decide if you need web search."
      },
      "memory_after": {
        "plan": "[OLLAMA ERROR] HTTPConnectionPool(host='localhost', port=11434): Read timed out. (read timeout=60) | PROMPT: You are a research agent. User query: Explain why world models are important for AI safety in simple terms.. Decide if you need web search.",
        "research_raw": "[SEARCH RESULTS for: Explain why world models are important for AI safety in simple terms.] ---> What are AI \u2018world models,\u2019 and why do they matter?\n\nWorld models, also known as world simulators, are being touted by some as the next big thing in AI.\n\nAI pioneer Fei-Fei Li\u2019s [World Labs](https://techcrunch.com/2024/09/13/fei-fei-lis-world-labs-comes-out-of-stealth-with-230m-in-funding/) has raised $230 million to build \u201clarge world models,\u201d and DeepMind [hired](https://techcrunch.com/2024/10/03/a-co-lead-on-sora-openais-video-generator-has-left-for-google/) one of the creators of OpenAI\u2019s video generator, [Sora](https://techcrunch.com/2024/02/15/openais-newest-model-can-generate-videos-and-they-look-decent/), to work on \u201cworld simulators.\u201d (Sora was released on Monday; [here are some early impressions](https://techcrunch.com/2024/12/09/openais-sora-is-launching-today-heres-highlights-from-the-first-review/).)\n\nBut what the heck *are*these things?\n\nWorld models take inspiration from the mental models of the world that humans develop naturally. Our brains take the abstract representations from our senses and form them into more concrete understanding of the world around us, producing what we called \u201cmodels\u201d long before AI adopted the phrase. The predictions our brains make based on these models influence how we perceive the world.\n\nA [paper](https://arxiv.org/pdf/1803.10122) by AI researchers David Ha and J\u00fcrgen Schmidhuber gives the example of a baseball batter. Batters have milliseconds to decide how to swing their bat \u2014 shorter than the time it takes for visual signals to reach the brain. The reason they\u2019re able to hit a 100-mile-per-hour fastball is because they can instinctively predict where the ball will go, Ha and Schmidhuber say.\n\n\u201cFor professional players, this all happens subconsciously,\u201d the research duo writes. \u201cTheir muscles reflexively swing the bat at the right time and location in line with their internal models\u2019 predictions. They can quickly act on their predictions of the future without the need to consciously roll out possible future scenarios to form a plan.\u201d\n\nIt\u2019s these subconscious reasoning aspects of world models that some believe are prerequisites for human-level intelligence.\n\nTechcrunch event\n\n### Join the Disrupt 2026 Waitlist\n\n#### Add yourself to the Disrupt 2026 waitlist to be first in line when Early Bird tickets drop. Past Disrupts have brought Google Cloud, Netflix, Microsoft, Box, Phia, a16z, ElevenLabs, Wayve, Hugging Face, Elad Gil, and Vinod Khosla to the stages \u2014 part of 250+ industry leaders driving 200+ sessions built to fuel your growth and sharpen your edge. Plus, meet the hundreds of startups innovating across every sector.\n\n### Join the Disrupt 2026 Waitlist\n\n#### Add yourself to the Disrupt 2026 waitlist to be first in line when Early Bird tickets drop. Past Disrupts have brought Google Cloud, Netflix, Microsoft, Box, Phia, a16z, ElevenLabs, Wayve, Hugging Face, Elad Gil, and Vinod Khosla to the stages \u2014 part of 250+ industry leaders driving 200+ sessions built to fuel your growth and sharpen your edge. Plus, meet the hundreds of startups innovating across every sector.\n\nSan Francisco | October 13-15, 2026\n\nWAITLIST NOW\n\n## Modeling the world\n\nWhile the concept has been around for decades, world models have gained popularity recently in part because of their promising applications in the field of generative video.\n\nMost, if not all, AI-generated videos veer into uncanny valley territory. Watch them long enough and somethingbizarrewill happen, like limbs twisting and merging into each other.\n\nWhile a generative model trained on years of video might accurately predict that a basketball bounces, it doesn\u2019t actually have any idea why \u2014 just like language models don\u2019t really understand the concepts behind words and phrases. But a world model with even a basic grasp of why the basketball bounces like it does will be better at showing it do that thing.\n\nTo enable this kind of insight, world models are trained on a range of data, including photos, audio, videos, and text, with the intent of creating internal representations of how the world works, and the ability to reason about the consequences of actions.\n\nA sample from AI startup Runway\u2019s Gen-3 video generation model.Image Credits:Runway\n\n\u201cA viewer expects that the world they\u2019re watching behaves in a similar way to their reality,\u201d Alex Mashrabov, Snap\u2019s ex-AI chief of AI and the CEO of [Higgsfield](https://techcrunch.com/2024/04/03/former-snap-ai-chief-launches-higgsfield-to-take-on-openais-sora-video-generator/), which is building generative models for video, said. \u201cIf a feather drops with the weight of an anvil or a bowling ball shoots up hundreds of feet into the air, it\u2019s jarring and takes the viewer out of the moment. With a strong world model, instead of a creator defining how each object is expected to move \u2014 which is tedious, cumbersome, and a poor use of time \u2014 the model will understand this.\u201d\n\nBut better video generation is only the tip of the iceberg for world models. Researchers including Meta chief AI scientist Yann LeCun say the models could someday be used for sophisticated forecasting and planning in both the digital and physical realm.\n\nIn a [talk](https://techcrunch.com/2024/10/16/metas-ai-chief-says-world-models-are-key-to-human-level-ai-but-it-might-be-10-years-out/) earlier this year, LeCun described how a world model could help achieve a desired goal through reasoning. A model with a base representation of a \u201cworld\u201d (e.g. a video of a dirty room), given an objective (a clean room), could come up with a sequence of actions to achieve that objective (deploy vacuums to sweep, clean the dishes, empty the trash) not because that\u2019s a pattern it has observed but because it knows at a deeper level how to go from dirty to clean.\n\n\u201cWe need machines that understand the world; [machines] that can remember things, that have intuition, have common sense \u2014 things that can reason and plan to the same level as humans,\u201d LeCun said. \u201cDespite what you might have heard from some of the most enthusiastic people, current AI systems are not capable of any of this.\u201d\n\nWhile LeCun estimates that we\u2019re at least a decade away from the world models he envisions, today\u2019s world models are showing promise as elementary physics simulators.\n\nSora controlling a player in Minecraft \u2014 and rendering the world.Image Credits:OpenAI\n\nOpenAI notes in a blog that Sora, which it considers to be a world model, can simulate actions like a painter leaving brush strokes on a canvas. Models like Sora \u2014 and Sora [itself](https://techcrunch.com/2024/02/15/openais-sora-video-generating-model-can-render-video-games-too/) \u2014 can also effectively [simulate](https://arxiv.org/abs/1803.10122) [video](https://arxiv.org/abs/2005.12126) [games](https://arstechnica.com/gadgets/2024/03/googles-genie-model-creates-interactive-2d-worlds-from-a-single-image/). For example, Sora can render a Minecraft-like UI and game world.\n\nFuture world models may be able to generate 3D worlds on demand for gaming, virtual photography, and more, World Labs co-founder Justin Johnson said on an [episode](https://a16z.com/podcast/the-frontier-of-spatial-intelligence-with-fei-fei-li/) of the a16z podcast.\n\n\u201cWe already have the ability to create virtual, interactive worlds, but it costs hundreds and hundreds of millions of dollars and a ton of development time,\u201d Johnson said. \u201c[World models] will let you not just get an image or a clip out, but a fully simulated, vibrant, and interactive 3D world.\u201d\n\n## High hurdles\n\nWhile the concept is enticing, many technical challenges stand in the way.\n\nTraining and running world models requires massive compute power even compared to the amount currently used by generative models. While some of the latest language models can run on a modern smartphone, Sora (arguably an early world model) would require thousands of GPUs to train and run, especially if their use becomes commonplace.\n\nWorld models, like all AI models, also [hallucinate](https://techcrunch.com/2024/08/14/study-suggests-that-even-the-best-ai-models-hallucinate-a-bunch/) \u2014 and internalize biases in their training data. A world model trained largely on videos of sunny weather in European cities might struggle to comprehend or depict Korean cities in snowy conditions, for example, or simply do so incorrectly.\n\nA general lack of training data threatens to exacerbate these issues, says Mashrabov.\n\n\u201cWe have seen models being really limited with generations of people of a certain type or race,\u201d he said. \u201cTraining data for a world model must be broad enough to cover a diverse set of scenarios, but also highly specific to where the AI can deeply understand the nuances of those scenarios.\u201d\n\nIn a recent [post](https://runwayml.com/research/introducing-general-world-models), AI startup Runway\u2019s CEO, Crist\u00f3bal Valenzuela, says that data and engineering issues prevent today\u2019s models from accurately capturing the behavior of a world\u2019s inhabitants (e.g. humans and animals). \u201cModels will need to generate consistent maps of the environment,\u201d he said, \u201cand the ability to navigate and interact in those environments.\u201d\n\nA Sora-generated video.Image Credits:OpenAI\n\nIf all the major hurdles are overcome, though, Mashrabov believes that world models could \u201cmore robustly\u201d bridge AI with the real world \u2014 leading to breakthroughs not only in virtual world generation but robotics and AI decision-making.\n\nThey could also spawn more capable robots.\n\nRobots today are limited in what they can do because they don\u2019t have an awareness of the world around them (or their own bodies). World models could give them that awareness, Mashrabov said \u2014 at least to a point.\n\n\u201cWith an advanced world model, an AI could develop a personal understanding of whatever scenario it\u2019s placed in,\u201d he said, \u201cand start to reason out possible solutions.\u201d\n\n*TechCrunch has an AI-focused newsletter!**Sign up here**to get it in your inbox every Wednesday.*\n\n*This story originally published October 28, 2024, and was updated December 14, 2024, with new updates about Sora.*\n\nTopics AI, Artificial Intelligence (AI), evergreens, world models\n\nKyle Wiggers AI Editor Kyle Wiggers was TechCrunch\u2019s AI Editor until June 2025. His writing has appeared in VentureBeat and Digital Trends, as well as a range of gadget blogs including Android Police, Android Authority, Droid-Life, and XDA-Developers. He lives in Manhattan with his partner, a music therapist. View Bio\n\nDecember 3, 2025 Palo Alto, CA StrictlyVC concludes its 2025 series with an exclusive event featuring insights from leading VCs and builders such as Pat Gelsinger, Mina Fahmi, and more. Plus, opportunities to forge meaningful connections.Early Bird rate ends November 17. Register Now\n\n## Most Popular\n\n- Leaked documents shed light into how much OpenAI pays Microsoft Rebecca Bellan - Spotify introduces a Premium Platinum plan with lossless access in five markets Ivan Mehta - Jack Dorsey funds diVine, a Vine reboot that includes Vine\u2019s video archive Sarah Perez - \u2018Chad: The Brainrot IDE\u2019 is a new Y Combinator-backed product so wild, people thought it was fake Julie Bort - Apple launches Digital ID, a way to carry your passport on your phone for use at TSA checkpoints Sarah Perez - Teradar raises $150M for a sensor it says beats lidar and radar Sean O'Kane - Meta\u2019s chief AI scientist Yann LeCun reportedly plans to leave to build his own startup Ram Iyer"
      }
    },
    {
      "step_id": 3,
      "node": "answer",
      "thought": "Generating final answer from context.",
      "tool": null,
      "tool_input": null,
      "tool_output": null,
      "memory_before": {
        "plan": "[OLLAMA ERROR] HTTPConnectionPool(host='localhost', port=11434): Read timed out. (read timeout=60) | PROMPT: You are a research agent. User query: Explain why world models are important for AI safety in simple terms.. Decide if you need web search.",
        "research_raw": "[SEARCH RESULTS for: Explain why world models are important for AI safety in simple terms.] ---> What are AI \u2018world models,\u2019 and why do they matter?\n\nWorld models, also known as world simulators, are being touted by some as the next big thing in AI.\n\nAI pioneer Fei-Fei Li\u2019s [World Labs](https://techcrunch.com/2024/09/13/fei-fei-lis-world-labs-comes-out-of-stealth-with-230m-in-funding/) has raised $230 million to build \u201clarge world models,\u201d and DeepMind [hired](https://techcrunch.com/2024/10/03/a-co-lead-on-sora-openais-video-generator-has-left-for-google/) one of the creators of OpenAI\u2019s video generator, [Sora](https://techcrunch.com/2024/02/15/openais-newest-model-can-generate-videos-and-they-look-decent/), to work on \u201cworld simulators.\u201d (Sora was released on Monday; [here are some early impressions](https://techcrunch.com/2024/12/09/openais-sora-is-launching-today-heres-highlights-from-the-first-review/).)\n\nBut what the heck *are*these things?\n\nWorld models take inspiration from the mental models of the world that humans develop naturally. Our brains take the abstract representations from our senses and form them into more concrete understanding of the world around us, producing what we called \u201cmodels\u201d long before AI adopted the phrase. The predictions our brains make based on these models influence how we perceive the world.\n\nA [paper](https://arxiv.org/pdf/1803.10122) by AI researchers David Ha and J\u00fcrgen Schmidhuber gives the example of a baseball batter. Batters have milliseconds to decide how to swing their bat \u2014 shorter than the time it takes for visual signals to reach the brain. The reason they\u2019re able to hit a 100-mile-per-hour fastball is because they can instinctively predict where the ball will go, Ha and Schmidhuber say.\n\n\u201cFor professional players, this all happens subconsciously,\u201d the research duo writes. \u201cTheir muscles reflexively swing the bat at the right time and location in line with their internal models\u2019 predictions. They can quickly act on their predictions of the future without the need to consciously roll out possible future scenarios to form a plan.\u201d\n\nIt\u2019s these subconscious reasoning aspects of world models that some believe are prerequisites for human-level intelligence.\n\nTechcrunch event\n\n### Join the Disrupt 2026 Waitlist\n\n#### Add yourself to the Disrupt 2026 waitlist to be first in line when Early Bird tickets drop. Past Disrupts have brought Google Cloud, Netflix, Microsoft, Box, Phia, a16z, ElevenLabs, Wayve, Hugging Face, Elad Gil, and Vinod Khosla to the stages \u2014 part of 250+ industry leaders driving 200+ sessions built to fuel your growth and sharpen your edge. Plus, meet the hundreds of startups innovating across every sector.\n\n### Join the Disrupt 2026 Waitlist\n\n#### Add yourself to the Disrupt 2026 waitlist to be first in line when Early Bird tickets drop. Past Disrupts have brought Google Cloud, Netflix, Microsoft, Box, Phia, a16z, ElevenLabs, Wayve, Hugging Face, Elad Gil, and Vinod Khosla to the stages \u2014 part of 250+ industry leaders driving 200+ sessions built to fuel your growth and sharpen your edge. Plus, meet the hundreds of startups innovating across every sector.\n\nSan Francisco | October 13-15, 2026\n\nWAITLIST NOW\n\n## Modeling the world\n\nWhile the concept has been around for decades, world models have gained popularity recently in part because of their promising applications in the field of generative video.\n\nMost, if not all, AI-generated videos veer into uncanny valley territory. Watch them long enough and somethingbizarrewill happen, like limbs twisting and merging into each other.\n\nWhile a generative model trained on years of video might accurately predict that a basketball bounces, it doesn\u2019t actually have any idea why \u2014 just like language models don\u2019t really understand the concepts behind words and phrases. But a world model with even a basic grasp of why the basketball bounces like it does will be better at showing it do that thing.\n\nTo enable this kind of insight, world models are trained on a range of data, including photos, audio, videos, and text, with the intent of creating internal representations of how the world works, and the ability to reason about the consequences of actions.\n\nA sample from AI startup Runway\u2019s Gen-3 video generation model.Image Credits:Runway\n\n\u201cA viewer expects that the world they\u2019re watching behaves in a similar way to their reality,\u201d Alex Mashrabov, Snap\u2019s ex-AI chief of AI and the CEO of [Higgsfield](https://techcrunch.com/2024/04/03/former-snap-ai-chief-launches-higgsfield-to-take-on-openais-sora-video-generator/), which is building generative models for video, said. \u201cIf a feather drops with the weight of an anvil or a bowling ball shoots up hundreds of feet into the air, it\u2019s jarring and takes the viewer out of the moment. With a strong world model, instead of a creator defining how each object is expected to move \u2014 which is tedious, cumbersome, and a poor use of time \u2014 the model will understand this.\u201d\n\nBut better video generation is only the tip of the iceberg for world models. Researchers including Meta chief AI scientist Yann LeCun say the models could someday be used for sophisticated forecasting and planning in both the digital and physical realm.\n\nIn a [talk](https://techcrunch.com/2024/10/16/metas-ai-chief-says-world-models-are-key-to-human-level-ai-but-it-might-be-10-years-out/) earlier this year, LeCun described how a world model could help achieve a desired goal through reasoning. A model with a base representation of a \u201cworld\u201d (e.g. a video of a dirty room), given an objective (a clean room), could come up with a sequence of actions to achieve that objective (deploy vacuums to sweep, clean the dishes, empty the trash) not because that\u2019s a pattern it has observed but because it knows at a deeper level how to go from dirty to clean.\n\n\u201cWe need machines that understand the world; [machines] that can remember things, that have intuition, have common sense \u2014 things that can reason and plan to the same level as humans,\u201d LeCun said. \u201cDespite what you might have heard from some of the most enthusiastic people, current AI systems are not capable of any of this.\u201d\n\nWhile LeCun estimates that we\u2019re at least a decade away from the world models he envisions, today\u2019s world models are showing promise as elementary physics simulators.\n\nSora controlling a player in Minecraft \u2014 and rendering the world.Image Credits:OpenAI\n\nOpenAI notes in a blog that Sora, which it considers to be a world model, can simulate actions like a painter leaving brush strokes on a canvas. Models like Sora \u2014 and Sora [itself](https://techcrunch.com/2024/02/15/openais-sora-video-generating-model-can-render-video-games-too/) \u2014 can also effectively [simulate](https://arxiv.org/abs/1803.10122) [video](https://arxiv.org/abs/2005.12126) [games](https://arstechnica.com/gadgets/2024/03/googles-genie-model-creates-interactive-2d-worlds-from-a-single-image/). For example, Sora can render a Minecraft-like UI and game world.\n\nFuture world models may be able to generate 3D worlds on demand for gaming, virtual photography, and more, World Labs co-founder Justin Johnson said on an [episode](https://a16z.com/podcast/the-frontier-of-spatial-intelligence-with-fei-fei-li/) of the a16z podcast.\n\n\u201cWe already have the ability to create virtual, interactive worlds, but it costs hundreds and hundreds of millions of dollars and a ton of development time,\u201d Johnson said. \u201c[World models] will let you not just get an image or a clip out, but a fully simulated, vibrant, and interactive 3D world.\u201d\n\n## High hurdles\n\nWhile the concept is enticing, many technical challenges stand in the way.\n\nTraining and running world models requires massive compute power even compared to the amount currently used by generative models. While some of the latest language models can run on a modern smartphone, Sora (arguably an early world model) would require thousands of GPUs to train and run, especially if their use becomes commonplace.\n\nWorld models, like all AI models, also [hallucinate](https://techcrunch.com/2024/08/14/study-suggests-that-even-the-best-ai-models-hallucinate-a-bunch/) \u2014 and internalize biases in their training data. A world model trained largely on videos of sunny weather in European cities might struggle to comprehend or depict Korean cities in snowy conditions, for example, or simply do so incorrectly.\n\nA general lack of training data threatens to exacerbate these issues, says Mashrabov.\n\n\u201cWe have seen models being really limited with generations of people of a certain type or race,\u201d he said. \u201cTraining data for a world model must be broad enough to cover a diverse set of scenarios, but also highly specific to where the AI can deeply understand the nuances of those scenarios.\u201d\n\nIn a recent [post](https://runwayml.com/research/introducing-general-world-models), AI startup Runway\u2019s CEO, Crist\u00f3bal Valenzuela, says that data and engineering issues prevent today\u2019s models from accurately capturing the behavior of a world\u2019s inhabitants (e.g. humans and animals). \u201cModels will need to generate consistent maps of the environment,\u201d he said, \u201cand the ability to navigate and interact in those environments.\u201d\n\nA Sora-generated video.Image Credits:OpenAI\n\nIf all the major hurdles are overcome, though, Mashrabov believes that world models could \u201cmore robustly\u201d bridge AI with the real world \u2014 leading to breakthroughs not only in virtual world generation but robotics and AI decision-making.\n\nThey could also spawn more capable robots.\n\nRobots today are limited in what they can do because they don\u2019t have an awareness of the world around them (or their own bodies). World models could give them that awareness, Mashrabov said \u2014 at least to a point.\n\n\u201cWith an advanced world model, an AI could develop a personal understanding of whatever scenario it\u2019s placed in,\u201d he said, \u201cand start to reason out possible solutions.\u201d\n\n*TechCrunch has an AI-focused newsletter!**Sign up here**to get it in your inbox every Wednesday.*\n\n*This story originally published October 28, 2024, and was updated December 14, 2024, with new updates about Sora.*\n\nTopics AI, Artificial Intelligence (AI), evergreens, world models\n\nKyle Wiggers AI Editor Kyle Wiggers was TechCrunch\u2019s AI Editor until June 2025. His writing has appeared in VentureBeat and Digital Trends, as well as a range of gadget blogs including Android Police, Android Authority, Droid-Life, and XDA-Developers. He lives in Manhattan with his partner, a music therapist. View Bio\n\nDecember 3, 2025 Palo Alto, CA StrictlyVC concludes its 2025 series with an exclusive event featuring insights from leading VCs and builders such as Pat Gelsinger, Mina Fahmi, and more. Plus, opportunities to forge meaningful connections.Early Bird rate ends November 17. Register Now\n\n## Most Popular\n\n- Leaked documents shed light into how much OpenAI pays Microsoft Rebecca Bellan - Spotify introduces a Premium Platinum plan with lossless access in five markets Ivan Mehta - Jack Dorsey funds diVine, a Vine reboot that includes Vine\u2019s video archive Sarah Perez - \u2018Chad: The Brainrot IDE\u2019 is a new Y Combinator-backed product so wild, people thought it was fake Julie Bort - Apple launches Digital ID, a way to carry your passport on your phone for use at TSA checkpoints Sarah Perez - Teradar raises $150M for a sensor it says beats lidar and radar Sean O'Kane - Meta\u2019s chief AI scientist Yann LeCun reportedly plans to leave to build his own startup Ram Iyer"
      },
      "memory_after": {
        "plan": "[OLLAMA ERROR] HTTPConnectionPool(host='localhost', port=11434): Read timed out. (read timeout=60) | PROMPT: You are a research agent. User query: Explain why world models are important for AI safety in simple terms.. Decide if you need web search.",
        "research_raw": "[SEARCH RESULTS for: Explain why world models are important for AI safety in simple terms.] ---> What are AI \u2018world models,\u2019 and why do they matter?\n\nWorld models, also known as world simulators, are being touted by some as the next big thing in AI.\n\nAI pioneer Fei-Fei Li\u2019s [World Labs](https://techcrunch.com/2024/09/13/fei-fei-lis-world-labs-comes-out-of-stealth-with-230m-in-funding/) has raised $230 million to build \u201clarge world models,\u201d and DeepMind [hired](https://techcrunch.com/2024/10/03/a-co-lead-on-sora-openais-video-generator-has-left-for-google/) one of the creators of OpenAI\u2019s video generator, [Sora](https://techcrunch.com/2024/02/15/openais-newest-model-can-generate-videos-and-they-look-decent/), to work on \u201cworld simulators.\u201d (Sora was released on Monday; [here are some early impressions](https://techcrunch.com/2024/12/09/openais-sora-is-launching-today-heres-highlights-from-the-first-review/).)\n\nBut what the heck *are*these things?\n\nWorld models take inspiration from the mental models of the world that humans develop naturally. Our brains take the abstract representations from our senses and form them into more concrete understanding of the world around us, producing what we called \u201cmodels\u201d long before AI adopted the phrase. The predictions our brains make based on these models influence how we perceive the world.\n\nA [paper](https://arxiv.org/pdf/1803.10122) by AI researchers David Ha and J\u00fcrgen Schmidhuber gives the example of a baseball batter. Batters have milliseconds to decide how to swing their bat \u2014 shorter than the time it takes for visual signals to reach the brain. The reason they\u2019re able to hit a 100-mile-per-hour fastball is because they can instinctively predict where the ball will go, Ha and Schmidhuber say.\n\n\u201cFor professional players, this all happens subconsciously,\u201d the research duo writes. \u201cTheir muscles reflexively swing the bat at the right time and location in line with their internal models\u2019 predictions. They can quickly act on their predictions of the future without the need to consciously roll out possible future scenarios to form a plan.\u201d\n\nIt\u2019s these subconscious reasoning aspects of world models that some believe are prerequisites for human-level intelligence.\n\nTechcrunch event\n\n### Join the Disrupt 2026 Waitlist\n\n#### Add yourself to the Disrupt 2026 waitlist to be first in line when Early Bird tickets drop. Past Disrupts have brought Google Cloud, Netflix, Microsoft, Box, Phia, a16z, ElevenLabs, Wayve, Hugging Face, Elad Gil, and Vinod Khosla to the stages \u2014 part of 250+ industry leaders driving 200+ sessions built to fuel your growth and sharpen your edge. Plus, meet the hundreds of startups innovating across every sector.\n\n### Join the Disrupt 2026 Waitlist\n\n#### Add yourself to the Disrupt 2026 waitlist to be first in line when Early Bird tickets drop. Past Disrupts have brought Google Cloud, Netflix, Microsoft, Box, Phia, a16z, ElevenLabs, Wayve, Hugging Face, Elad Gil, and Vinod Khosla to the stages \u2014 part of 250+ industry leaders driving 200+ sessions built to fuel your growth and sharpen your edge. Plus, meet the hundreds of startups innovating across every sector.\n\nSan Francisco | October 13-15, 2026\n\nWAITLIST NOW\n\n## Modeling the world\n\nWhile the concept has been around for decades, world models have gained popularity recently in part because of their promising applications in the field of generative video.\n\nMost, if not all, AI-generated videos veer into uncanny valley territory. Watch them long enough and somethingbizarrewill happen, like limbs twisting and merging into each other.\n\nWhile a generative model trained on years of video might accurately predict that a basketball bounces, it doesn\u2019t actually have any idea why \u2014 just like language models don\u2019t really understand the concepts behind words and phrases. But a world model with even a basic grasp of why the basketball bounces like it does will be better at showing it do that thing.\n\nTo enable this kind of insight, world models are trained on a range of data, including photos, audio, videos, and text, with the intent of creating internal representations of how the world works, and the ability to reason about the consequences of actions.\n\nA sample from AI startup Runway\u2019s Gen-3 video generation model.Image Credits:Runway\n\n\u201cA viewer expects that the world they\u2019re watching behaves in a similar way to their reality,\u201d Alex Mashrabov, Snap\u2019s ex-AI chief of AI and the CEO of [Higgsfield](https://techcrunch.com/2024/04/03/former-snap-ai-chief-launches-higgsfield-to-take-on-openais-sora-video-generator/), which is building generative models for video, said. \u201cIf a feather drops with the weight of an anvil or a bowling ball shoots up hundreds of feet into the air, it\u2019s jarring and takes the viewer out of the moment. With a strong world model, instead of a creator defining how each object is expected to move \u2014 which is tedious, cumbersome, and a poor use of time \u2014 the model will understand this.\u201d\n\nBut better video generation is only the tip of the iceberg for world models. Researchers including Meta chief AI scientist Yann LeCun say the models could someday be used for sophisticated forecasting and planning in both the digital and physical realm.\n\nIn a [talk](https://techcrunch.com/2024/10/16/metas-ai-chief-says-world-models-are-key-to-human-level-ai-but-it-might-be-10-years-out/) earlier this year, LeCun described how a world model could help achieve a desired goal through reasoning. A model with a base representation of a \u201cworld\u201d (e.g. a video of a dirty room), given an objective (a clean room), could come up with a sequence of actions to achieve that objective (deploy vacuums to sweep, clean the dishes, empty the trash) not because that\u2019s a pattern it has observed but because it knows at a deeper level how to go from dirty to clean.\n\n\u201cWe need machines that understand the world; [machines] that can remember things, that have intuition, have common sense \u2014 things that can reason and plan to the same level as humans,\u201d LeCun said. \u201cDespite what you might have heard from some of the most enthusiastic people, current AI systems are not capable of any of this.\u201d\n\nWhile LeCun estimates that we\u2019re at least a decade away from the world models he envisions, today\u2019s world models are showing promise as elementary physics simulators.\n\nSora controlling a player in Minecraft \u2014 and rendering the world.Image Credits:OpenAI\n\nOpenAI notes in a blog that Sora, which it considers to be a world model, can simulate actions like a painter leaving brush strokes on a canvas. Models like Sora \u2014 and Sora [itself](https://techcrunch.com/2024/02/15/openais-sora-video-generating-model-can-render-video-games-too/) \u2014 can also effectively [simulate](https://arxiv.org/abs/1803.10122) [video](https://arxiv.org/abs/2005.12126) [games](https://arstechnica.com/gadgets/2024/03/googles-genie-model-creates-interactive-2d-worlds-from-a-single-image/). For example, Sora can render a Minecraft-like UI and game world.\n\nFuture world models may be able to generate 3D worlds on demand for gaming, virtual photography, and more, World Labs co-founder Justin Johnson said on an [episode](https://a16z.com/podcast/the-frontier-of-spatial-intelligence-with-fei-fei-li/) of the a16z podcast.\n\n\u201cWe already have the ability to create virtual, interactive worlds, but it costs hundreds and hundreds of millions of dollars and a ton of development time,\u201d Johnson said. \u201c[World models] will let you not just get an image or a clip out, but a fully simulated, vibrant, and interactive 3D world.\u201d\n\n## High hurdles\n\nWhile the concept is enticing, many technical challenges stand in the way.\n\nTraining and running world models requires massive compute power even compared to the amount currently used by generative models. While some of the latest language models can run on a modern smartphone, Sora (arguably an early world model) would require thousands of GPUs to train and run, especially if their use becomes commonplace.\n\nWorld models, like all AI models, also [hallucinate](https://techcrunch.com/2024/08/14/study-suggests-that-even-the-best-ai-models-hallucinate-a-bunch/) \u2014 and internalize biases in their training data. A world model trained largely on videos of sunny weather in European cities might struggle to comprehend or depict Korean cities in snowy conditions, for example, or simply do so incorrectly.\n\nA general lack of training data threatens to exacerbate these issues, says Mashrabov.\n\n\u201cWe have seen models being really limited with generations of people of a certain type or race,\u201d he said. \u201cTraining data for a world model must be broad enough to cover a diverse set of scenarios, but also highly specific to where the AI can deeply understand the nuances of those scenarios.\u201d\n\nIn a recent [post](https://runwayml.com/research/introducing-general-world-models), AI startup Runway\u2019s CEO, Crist\u00f3bal Valenzuela, says that data and engineering issues prevent today\u2019s models from accurately capturing the behavior of a world\u2019s inhabitants (e.g. humans and animals). \u201cModels will need to generate consistent maps of the environment,\u201d he said, \u201cand the ability to navigate and interact in those environments.\u201d\n\nA Sora-generated video.Image Credits:OpenAI\n\nIf all the major hurdles are overcome, though, Mashrabov believes that world models could \u201cmore robustly\u201d bridge AI with the real world \u2014 leading to breakthroughs not only in virtual world generation but robotics and AI decision-making.\n\nThey could also spawn more capable robots.\n\nRobots today are limited in what they can do because they don\u2019t have an awareness of the world around them (or their own bodies). World models could give them that awareness, Mashrabov said \u2014 at least to a point.\n\n\u201cWith an advanced world model, an AI could develop a personal understanding of whatever scenario it\u2019s placed in,\u201d he said, \u201cand start to reason out possible solutions.\u201d\n\n*TechCrunch has an AI-focused newsletter!**Sign up here**to get it in your inbox every Wednesday.*\n\n*This story originally published October 28, 2024, and was updated December 14, 2024, with new updates about Sora.*\n\nTopics AI, Artificial Intelligence (AI), evergreens, world models\n\nKyle Wiggers AI Editor Kyle Wiggers was TechCrunch\u2019s AI Editor until June 2025. His writing has appeared in VentureBeat and Digital Trends, as well as a range of gadget blogs including Android Police, Android Authority, Droid-Life, and XDA-Developers. He lives in Manhattan with his partner, a music therapist. View Bio\n\nDecember 3, 2025 Palo Alto, CA StrictlyVC concludes its 2025 series with an exclusive event featuring insights from leading VCs and builders such as Pat Gelsinger, Mina Fahmi, and more. Plus, opportunities to forge meaningful connections.Early Bird rate ends November 17. Register Now\n\n## Most Popular\n\n- Leaked documents shed light into how much OpenAI pays Microsoft Rebecca Bellan - Spotify introduces a Premium Platinum plan with lossless access in five markets Ivan Mehta - Jack Dorsey funds diVine, a Vine reboot that includes Vine\u2019s video archive Sarah Perez - \u2018Chad: The Brainrot IDE\u2019 is a new Y Combinator-backed product so wild, people thought it was fake Julie Bort - Apple launches Digital ID, a way to carry your passport on your phone for use at TSA checkpoints Sarah Perez - Teradar raises $150M for a sensor it says beats lidar and radar Sean O'Kane - Meta\u2019s chief AI scientist Yann LeCun reportedly plans to leave to build his own startup Ram Iyer"
      }
    }
  ],
  "final_answer": "[OLLAMA ERROR] HTTPConnectionPool(host='localhost', port=11434): Read timed out. (read timeout=60) | PROMPT: User query: Explain why world models are important for AI safety in simple terms.\nContext: [SEARCH RESULTS for: Explain why world models are important for AI safety in simple terms.] ---> What are AI "
}